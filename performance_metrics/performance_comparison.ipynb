{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104f34bf",
   "metadata": {},
   "source": [
    "# Compare performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b326d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-95a1bff4e169>:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, auc, roc_curve\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912b559",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f711010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(d):\n",
    "    l = [ v for k,v in d.items() ]\n",
    "    l = [item for sublist in l for item in sublist]\n",
    "    l = list(map(int, l))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad4f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_fine(l):\n",
    "    binarized_labels = []\n",
    "    \n",
    "    for i in l:\n",
    "        if i > 1:\n",
    "            i = 1\n",
    "            binarized_labels.append( i )\n",
    "        else:\n",
    "            binarized_labels.append( i )\n",
    "    \n",
    "    return binarized_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991d99c",
   "metadata": {},
   "source": [
    "## groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d26de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load groundtruth for coarse matches\n",
    "\n",
    "gt_file = '/mnt/nas2/results/Results/systematicReview/order_free_matching/EBM_PICO_training_matches/order_bound/lf_Therapeutic_or_Preventive_Procedure.tsv'\n",
    "\n",
    "# open file and load into a dataframe\n",
    "groundtruth_df = pd.read_csv(gt_file, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d295e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the coarse and fine and add the pmid:gt to a ob dictionaries\n",
    "gt_coarse = dict()\n",
    "gt_fine = dict()\n",
    "\n",
    "for identifier, x, y in zip(groundtruth_df['pmid'], groundtruth_df['i'], groundtruth_df['i_f']):\n",
    "    x = ast.literal_eval(x)\n",
    "    y = ast.literal_eval(y)\n",
    "    \n",
    "    gt_coarse[str(identifier)] = x\n",
    "    gt_fine[str(identifier)] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de4bdf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of PMIDs for groundtruth - coarse:  4802\n",
      "Total number of PMIDs for groundtruth - fine:  4802\n"
     ]
    }
   ],
   "source": [
    "print( 'Total number of PMIDs for groundtruth - coarse: ', len(gt_coarse) )\n",
    "print( 'Total number of PMIDs for groundtruth - fine: ', len(gt_fine) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66d2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten groundtruth\n",
    "\n",
    "gt_coarse_flattened = flatten( gt_coarse )\n",
    "gt_fine_flattened = flatten( gt_fine )\n",
    "\n",
    "# binarize the fine groundtruth labels\n",
    "gt_fine_flattened = binarize_fine(gt_fine_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7692e4",
   "metadata": {},
   "source": [
    "## order-bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51adfcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load order bound matches for all dictionaries\n",
    "\n",
    "ob_dir = '/mnt/nas2/results/Results/systematicReview/order_free_matching/EBM_PICO_training_matches/order_bound'\n",
    "ob_files = os.listdir(ob_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93491fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:42<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "ob_matches = dict()\n",
    "\n",
    "for i in tqdm(ob_files):\n",
    "    file_path = f'{ob_dir}/{i}'\n",
    "    \n",
    "    filename = str(i).replace('lf_', '')\n",
    "    filename = str(filename).replace('.tsv', '')\n",
    "    \n",
    "    # open file and load into a dataframe\n",
    "    data_df = pd.read_csv(file_path, sep='\\t', header=0)\n",
    "    \n",
    "    # process the labels and add the pmid:labels to a ob dictionary\n",
    "    labs_all = []\n",
    "    for x in data_df['labels']:\n",
    "        x = ast.literal_eval(x)\n",
    "        labs_all.append( x )\n",
    "    ob_matches[filename] = dict( zip( data_df['pmid'],  pd.Series( labs_all )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a18cfd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of PMIDs for orderbound files:  4802\n"
     ]
    }
   ],
   "source": [
    "print( 'Total number of PMIDs for orderbound files: ', len(ob_matches['Therapeutic_or_Preventive_Procedure']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd75185",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(gt_fine) == len( ob_matches['Therapeutic_or_Preventive_Procedure'] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0ea58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for coarse-grained ground truth and Medical_Device matches\n",
      "0.5084, 0.4959, 0.9048, 0.9917, 0.9463, 0.2454, 0.0251, 0.0455\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Medical_Device matches\n",
      "0.5108, 0.5067, 0.9321, 0.9916, 0.9610, 0.2103, 0.0300, 0.0525\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Laboratory_Procedure matches\n",
      "0.5155, 0.5110, 0.9061, 0.9841, 0.9435, 0.2400, 0.0469, 0.0785\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Laboratory_Procedure matches\n",
      "0.5207, 0.5241, 0.9335, 0.9840, 0.9580, 0.2105, 0.0574, 0.0903\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Manufactured_Object matches\n",
      "0.5038, 0.4921, 0.9040, 0.9821, 0.9414, 0.1320, 0.0254, 0.0427\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Manufactured_Object matches\n",
      "0.5061, 0.5018, 0.9315, 0.9822, 0.9562, 0.1117, 0.0300, 0.0474\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Pharmacologic_Substance matches\n",
      "0.5715, 0.5919, 0.9163, 0.9742, 0.9444, 0.4122, 0.1688, 0.2395\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Pharmacologic_Substance matches\n",
      "0.5927, 0.6140, 0.9432, 0.9733, 0.9580, 0.3714, 0.2122, 0.2701\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Diagnostic_Procedure matches\n",
      "0.5024, 0.4871, 0.9038, 0.9877, 0.9439, 0.1302, 0.0171, 0.0303\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Diagnostic_Procedure matches\n",
      "0.5037, 0.4960, 0.9312, 0.9878, 0.9587, 0.1072, 0.0197, 0.0333\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Therapeutic_or_Preventive_Procedure matches\n",
      "0.5460, 0.5563, 0.9117, 0.9656, 0.9379, 0.2821, 0.1265, 0.1747\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Therapeutic_or_Preventive_Procedure matches\n",
      "0.5588, 0.5698, 0.9387, 0.9648, 0.9516, 0.2443, 0.1529, 0.1881\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Organic_Chemical matches\n",
      "0.5577, 0.5757, 0.9137, 0.9820, 0.9466, 0.4416, 0.1334, 0.2049\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Organic_Chemical matches\n",
      "0.5750, 0.5989, 0.9407, 0.9812, 0.9605, 0.4003, 0.1687, 0.2374\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Biologically_Active_Substance matches\n",
      "0.5072, 0.4925, 0.9046, 0.9940, 0.9472, 0.2676, 0.0204, 0.0378\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Biologically_Active_Substance matches\n",
      "0.5094, 0.5034, 0.9320, 0.9939, 0.9620, 0.2339, 0.0248, 0.0449\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Idea_or_Concept matches\n",
      "0.4853, 0.4824, 0.9006, 0.9249, 0.9126, 0.0610, 0.0456, 0.0522\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Idea_or_Concept matches\n",
      "0.4855, 0.4858, 0.9287, 0.9257, 0.9272, 0.0434, 0.0453, 0.0443\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Professional_Society matches\n",
      "0.5000, 0.4746, 0.9033, 1.0000, 0.9492, 0.2000, 0.0000, 0.0000\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Professional_Society matches\n",
      "0.5000, 0.4821, 0.9307, 1.0000, 0.9641, 0.0000, 0.0000, 0.0000\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Temporal_Concept matches\n",
      "0.5009, 0.4955, 0.9035, 0.9581, 0.9300, 0.1006, 0.0438, 0.0610\n"
     ]
    }
   ],
   "source": [
    "# calculate performance metrics for order-bound files\n",
    "\n",
    "for k, v in ob_matches.items():\n",
    "    \n",
    "    v_flattened = flatten( v )\n",
    "    \n",
    "    # Classification report\n",
    "    cr_order_bound_coarse = classification_report( gt_coarse_flattened, v_flattened, digits=4, output_dict=True)\n",
    "    print(f'Confusion matrix for coarse-grained ground truth and {k} matches')\n",
    "    report = '{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(cr_order_bound_coarse['macro avg']['recall'], cr_order_bound_coarse['macro avg']['f1-score'], cr_order_bound_coarse['0']['precision'], cr_order_bound_coarse['0']['recall'], cr_order_bound_coarse['0']['f1-score'], cr_order_bound_coarse['1']['precision'], cr_order_bound_coarse['1']['recall'], cr_order_bound_coarse['1']['f1-score'])\n",
    "    print( report )\n",
    "    \n",
    "\n",
    "    cr_order_bound_fine = classification_report( gt_fine_flattened, v_flattened, digits=4, output_dict=True)\n",
    "    print(f'\\n\\nConfusion matrix for fine-grained ground truth and {k} matches')\n",
    "    report = '{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(cr_order_bound_fine['macro avg']['recall'], cr_order_bound_fine['macro avg']['f1-score'], cr_order_bound_fine['0']['precision'], cr_order_bound_fine['0']['recall'], cr_order_bound_fine['0']['f1-score'], cr_order_bound_fine['1']['precision'], cr_order_bound_fine['1']['recall'], cr_order_bound_fine['1']['f1-score'])\n",
    "    print( report )\n",
    "    \n",
    "    print( '--------------------------------------------------------------------------' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0af70",
   "metadata": {},
   "source": [
    "## order-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb32581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load order free matches with all dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50970c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78eab225",
   "metadata": {},
   "source": [
    "## actual order-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6feda7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (actual) order free matches with all dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c1783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
