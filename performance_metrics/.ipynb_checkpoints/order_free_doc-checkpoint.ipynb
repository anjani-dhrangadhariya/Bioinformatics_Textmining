{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59401921",
   "metadata": {},
   "source": [
    "# Order-free matching performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89e1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import ast\n",
    "import json\n",
    "\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, auc, roc_curve\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52458f3",
   "metadata": {},
   "source": [
    "## Read order-free candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9a9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "picos = 'I' # ['P', 'I', 'O', 'S']\n",
    "match_level = 'doc' # ['doc', 'sent', 'win_5', 'para']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe89aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:  ['Biomedical_or_Dental_Material.json', '.nfs0000000011200a4200000003', 'Classification.json', 'Intellectual_Product.json', 'Biologically_Active_Substance.json', 'Diagnostic_Procedure.json', 'Gene_or_Genome.json', 'Finding.json', 'Functional_Concept.json', 'Medical_Device.json', 'Organic_Chemical.json', 'Laboratory__Procedure.json', 'Manufactured_Object.json', 'train_ebm_intervention.json', 'Professional_Society.json', 'Pharmacologic_Substance.json', 'Therapeutic_or_Preventive_Procedure.json', 'train_ebm_intervention_syn.json', 'Biomedical_Occupation_or_Discipline.json', 'Health_Care_Activity.json', 'Idea_or_Concept.json', 'Temporal_Concept.json']\n"
     ]
    }
   ],
   "source": [
    "order_free_dir = f'/mnt/nas2/results/Results/systematicReview/order_free_matching/EBM_PICO_training_matches/order_free/{match_level}/{picos}'\n",
    "order_free_files = os.listdir(order_free_dir)\n",
    "print('Files: ', order_free_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1012ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_free_files.remove('.nfs0000000011200a4200000003')\n",
    "#order_free_files.remove('Finding.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822093ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Biomedical_or_Dental_Material.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 1/21 [00:00<00:15,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Classification.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 2/21 [00:01<00:11,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Intellectual_Product.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 3/21 [00:16<02:09,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Biologically_Active_Substance.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 4/21 [00:20<01:41,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Diagnostic_Procedure.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 5/21 [00:44<03:20, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Gene_or_Genome.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 6/21 [00:45<02:08,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Finding.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 7/21 [02:17<08:22, 35.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Functional_Concept.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 8/21 [02:17<05:19, 24.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Medical_Device.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 9/21 [02:20<03:33, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Organic_Chemical.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 10/21 [02:22<02:19, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Laboratory__Procedure.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 11/21 [02:51<02:58, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Manufactured_Object.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 12/21 [02:52<01:52, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... train_ebm_intervention.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 13/21 [03:00<01:31, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Professional_Society.json\n",
      "Loading file... Pharmacologic_Substance.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 15/21 [03:04<00:41,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Therapeutic_or_Preventive_Procedure.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 16/21 [06:17<04:25, 53.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... train_ebm_intervention_syn.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 17/21 [06:19<02:39, 39.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Biomedical_Occupation_or_Discipline.json\n",
      "Loading file... Health_Care_Activity.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [07:34<00:29, 29.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Idea_or_Concept.json\n",
      "Loading file... Temporal_Concept.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [07:35<00:00, 21.67s/it]\n"
     ]
    }
   ],
   "source": [
    "orf_loaded_files = dict()\n",
    "\n",
    "for i in tqdm(order_free_files):\n",
    "    \n",
    "    filpath = f'{order_free_dir}/{i}'\n",
    "    print('Loading file...', i)\n",
    "    with open( filpath, 'r' ) as rf:\n",
    "        orf_i = json.load(rf)\n",
    "        orf_loaded_files[i] = orf_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482441f",
   "metadata": {},
   "source": [
    "## Load order-free candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6613092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_n_consecutive(numbers, n):\n",
    "    n = 2\n",
    "    if len(numbers) < n:\n",
    "        return False\n",
    "    for i in range(len(numbers) - n + 1):\n",
    "        window = numbers[i:i+n]\n",
    "        if max(window) - min(window) != n-1:\n",
    "            continue\n",
    "        if len(set(window)) == n:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d7c5ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "\n",
    "set_partial_matches = True\n",
    "set_actual_orf_matches = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3cf821f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orf(v, par, actual_orf):\n",
    "    \n",
    "    orfs = dict()\n",
    "    \n",
    "    for k_i, v_i in v.items():\n",
    "        # print( k_i ) # example: name_15_3, name_9_9, name_12_8\n",
    "        \n",
    "        if 'Inters. (full)' in v_i and len(v_i['Inters. (full)']) > 0:\n",
    "            \n",
    "            full_inters = v_i['Inters. (full)']\n",
    "            # full_inters.keys() = PMIDs\n",
    "            # full_inters.values() = offsets, tokens\n",
    "\n",
    "            for pmid, matches in full_inters.items():\n",
    "                if len(matches['char offs.']) > 1:\n",
    "                    if pmid not in orfs:\n",
    "                        if actual_orf == True:\n",
    "                            #if sorted( matches['word offs.'] ) != matches['word offs.']:\n",
    "                            if are_n_consecutive(matches['word offs.'], 2) == False:\n",
    "                                orfs[pmid] = [ ]\n",
    "                                orfs[pmid].extend( matches['word offs.'] )      \n",
    "                        elif actual_orf == False: \n",
    "                            orfs[pmid] = [ ]\n",
    "                            orfs[pmid].extend( matches['word offs.'] )\n",
    "                    else:\n",
    "                        if actual_orf == True:\n",
    "                            #if sorted( matches['word offs.'] ) != matches['word offs.']:\n",
    "                            if are_n_consecutive(matches['word offs.'], 2) == False:\n",
    "                                orfs[pmid].extend( matches['word offs.'] )\n",
    "                        elif actual_orf == False:\n",
    "                            orfs[pmid].extend( matches['word offs.'] )\n",
    "\n",
    "\n",
    "        if 'Inters. (partial)' in v_i and len(v_i['Inters. (partial)']) > 0 and (par=='both' or par==True):\n",
    "            \n",
    "            par_inters = v_i['Inters. (partial)']\n",
    "            \n",
    "            for pmid, matches in par_inters.items():\n",
    "                if len(matches['char offs.']) > 1:\n",
    "                    if pmid not in orfs:\n",
    "                        if actual_orf == True:\n",
    "                            #if sorted( matches['word offs.'] ) != matches['word offs.']:\n",
    "                            if are_n_consecutive(matches['word offs.'], 2) == False:\n",
    "                                orfs[pmid] = [ ]\n",
    "                                orfs[pmid].extend( matches['word offs.'] )   \n",
    "                        elif actual_orf == False:\n",
    "                            orfs[pmid] = [ ]\n",
    "                            orfs[pmid].extend( matches['word offs.'] ) \n",
    "                    else:\n",
    "                        if actual_orf == True:\n",
    "                            #if sorted( matches['word offs.'] ) != matches['word offs.']:\n",
    "                            if are_n_consecutive(matches['word offs.'], 2) == False:\n",
    "                                orfs[pmid].extend( matches['word offs.'] )\n",
    "                        elif actual_orf == False:\n",
    "                            orfs[pmid].extend( matches['word offs.'] )\n",
    "    \n",
    "    return orfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "orfs_dict = dict()\n",
    "\n",
    "for k,v in orf_loaded_files.items():\n",
    "\n",
    "    orf_fetched = get_orf(v, par = set_partial_matches, actual_orf = set_actual_orf_matches)\n",
    "    orfs_dict[k] = orf_fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the loaded ORF matches in ascending order depending on the number of matches found\n",
    "\n",
    "order_offset_dict = dict()\n",
    "\n",
    "for k,v in orfs_dict.items():\n",
    "    \n",
    "    total_offsets = []\n",
    "    for k_i, v_i in v.items():\n",
    "        total_offsets.extend( v_i )\n",
    "    \n",
    "    order_offset_dict[k] = len(total_offsets)\n",
    "    \n",
    "    print( f'Offsets in {k}: ', len(total_offsets) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dictionary\n",
    "order_offset_sorteddict = dict(sorted(order_offset_dict.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3616407",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Total number of dictionaries loaded: ', len(order_offset_sorteddict) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12e929",
   "metadata": {},
   "source": [
    "## Load order-bound matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afaa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_free_matches(x, orf_offsets):\n",
    "    \n",
    "    labs_modified = []\n",
    "    \n",
    "    for i, (identifier, offs, labs) in enumerate( zip(x.pmid, x.offsets, x.labels) ):\n",
    "             \n",
    "        lab_val = [v for k, v in ast.literal_eval(labs).items()] \n",
    "        off_val = ast.literal_eval(offs) \n",
    "        \n",
    "        if str(identifier) in orf_offsets: \n",
    "            orf_matches =  orf_offsets[ str(identifier) ]\n",
    "            match_indices = [ off_val.index(m) for m in orf_matches ]\n",
    "            for i, l in enumerate(lab_val):\n",
    "                if i in match_indices:\n",
    "                    lab_val[i] = 1\n",
    "                    \n",
    "        labs_modified.append( lab_val )\n",
    "        \n",
    "        \n",
    "    return labs_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_int = f'/mnt/nas2/results/Results/systematicReview/order_free_matching/EBM_PICO_training_matches/direct/{picos}/lf_ds_intervention_syn.tsv'\n",
    "ob_int_syn = f'/mnt/nas2/results/Results/systematicReview/order_free_matching/EBM_PICO_training_matches/direct/{picos}/lf_ds_intervetion.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6796b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_int_df = pd.read_csv(ob_int, sep='\\t', header=0)\n",
    "ob_int_syn_df = pd.read_csv(ob_int_syn, sep='\\t', header=0)\n",
    "ob_merged_df = pd.concat([ob_int_df,ob_int_syn_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f85f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gt(l):\n",
    "    \n",
    "    labels = l\n",
    "    \n",
    "    if isinstance(labels, str):\n",
    "        labels = ast.literal_eval(labels)\n",
    "        \n",
    "    # convert non-1 fine labels labels to 1's\n",
    "    labels = ['1' if (n != '1' and n != '0') else str(n) for i, n in enumerate(labels) ]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "ob_int_df['i'] = ob_int_df.i.apply(process_gt)\n",
    "ob_int_syn_df['i'] = ob_int_syn_df.i.apply(process_gt)\n",
    "\n",
    "ob_int_df['i_f'] = ob_int_df.i_f.apply(process_gt)\n",
    "ob_int_syn_df['i_f'] = ob_int_syn_df.i_f.apply(process_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803aa5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch ground truth from the direct matching\n",
    "\n",
    "coarse_int_gt = dict(zip(ob_int_df['pmid'], ob_int_df['i']))\n",
    "fine_int_gt = dict(zip(ob_int_df['pmid'], ob_int_df['i_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess order-bound labels\n",
    "\n",
    "def process_ob_labs(l):\n",
    "    \n",
    "    labels = l\n",
    "    \n",
    "    if isinstance( labels, str ):\n",
    "        labels = ast.literal_eval(labels)\n",
    "\n",
    "    labels = [ v for k, v in labels.items() ]\n",
    "    labels = ['0' if n == -1 else str(n) for i, n in enumerate(labels) ]\n",
    "\n",
    "    return labels\n",
    "\n",
    "ob_int_df['labels'] = ob_int_df.labels.apply(process_ob_labs) # order bound matching labels for int source\n",
    "ob_int_syn_df['labels'] = ob_int_syn_df.labels.apply(process_ob_labs) # order bound matching labels for int_syn source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch order-bound predictions for merged dataframes\n",
    "\n",
    "ob_preds_merged = dict()\n",
    "\n",
    "ob_int_dict = dict(zip(ob_int_df['pmid'], ob_int_df['labels']))\n",
    "ob_int_syn_dict = dict(zip(ob_int_syn_df['pmid'], ob_int_syn_df['labels']))\n",
    "\n",
    "for k,v in ob_int_dict.items():\n",
    "\n",
    "    if k not in ob_preds_merged:\n",
    "        ob_preds_merged[k] = []\n",
    "        ob_preds_merged[k] = v\n",
    "\n",
    "    else:\n",
    "        old_pred = ob_preds_merged[k]\n",
    "        new_pred = v\n",
    "        \n",
    "        # merge old and new predictions\n",
    "        merged_predictions = [ max( o,n ) for o,n in zip( old_pred, new_pred ) ]\n",
    "        assert len( old_pred ) == len( new_pred ) == len( merged_predictions )\n",
    "        ob_preds_merged[k] = merged_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len( list(ob_preds_merged.values()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cceae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ob_int_syn_dict.items():\n",
    "\n",
    "    if k not in ob_preds_merged:\n",
    "        ob_preds_merged[k] = []\n",
    "        ob_preds_merged[k] = v\n",
    "\n",
    "    else:\n",
    "        old_pred = ob_preds_merged[k]\n",
    "        new_pred = v\n",
    "\n",
    "        # merge old and new predictions\n",
    "        #print( 'merging the new predictions...' )\n",
    "        merged_predictions = [ max( o,n ) for o,n in zip( old_pred, new_pred ) ]\n",
    "        assert len( old_pred ) == len( new_pred ) == len( merged_predictions )\n",
    "        ob_preds_merged[k] = merged_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7b1dd",
   "metadata": {},
   "source": [
    "## merge and calculate evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61079a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(d):\n",
    "    l = [ v for k,v in d.items() ]\n",
    "    l = [item for sublist in l for item in sublist]\n",
    "    l = list(map(int, l))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth : coarse_int_gt, fine_int_gt\n",
    "# ob/direct matching preds :  ob_preds_merged\n",
    "\n",
    "order_bound_preds = flatten( ob_preds_merged )\n",
    "picos_coarse = flatten( coarse_int_gt )\n",
    "picos_fine = flatten( fine_int_gt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_preds_ordered( ob_preds, orf_offsets ):\n",
    "    \n",
    "    merged_predictions = dict()\n",
    "    \n",
    "    for k, v in ob_preds.items(): # ob preds are the old preds\n",
    "        \n",
    "        k = str(k)\n",
    "\n",
    "        if k in orf_offsets:\n",
    "            matching_offsets = orf_offsets[k]\n",
    "            old_preds = list( map( int, v ))\n",
    "            new_preds = list( map( int, v ))\n",
    "                             \n",
    "            # add new offsets to the new_preds\n",
    "            for indice in matching_offsets:\n",
    "                new_preds[indice] = 1\n",
    "\n",
    "            merged_predictions[ k ] = new_preds\n",
    "        else:\n",
    "            merged_predictions[ k ] = list( map( int, v ) )\n",
    "    \n",
    "    return merged_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ccfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_orf_ordered = list(order_offset_sorteddict.keys())\n",
    "all_orf_ordered.insert( 0, 'order bound' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_preds = dict()\n",
    "base_preds_flattened = []\n",
    "\n",
    "\n",
    "for count, i in enumerate(all_orf_ordered):\n",
    "    \n",
    "    # get offsets and merge with the ob ones\n",
    "    if count == 0:\n",
    "        base_preds = ob_preds_merged\n",
    "        base_preds_flattened =  flatten( ob_preds_merged )\n",
    "    else:\n",
    "        # Add more dicts to orderbound preds and modify base_preds_flattened\n",
    "        updated_of_preds = merge_preds_ordered( base_preds, orfs_dict[i] )\n",
    "        base_preds_flattened =  flatten( updated_of_preds )\n",
    "        base_preds = updated_of_preds\n",
    "    \n",
    "    \n",
    "    # Classification report\n",
    "    cr_order_bound_coarse = classification_report( picos_coarse, base_preds_flattened, digits=4, output_dict=True)\n",
    "    print(f'Confusion matrix for coarse-grained ground truth and {i} matches')\n",
    "    report = '{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(cr_order_bound_coarse['macro avg']['recall'], cr_order_bound_coarse['macro avg']['f1-score'], cr_order_bound_coarse['0']['precision'], cr_order_bound_coarse['0']['recall'], cr_order_bound_coarse['0']['f1-score'], cr_order_bound_coarse['1']['precision'], cr_order_bound_coarse['1']['recall'], cr_order_bound_coarse['1']['f1-score'])\n",
    "    print( report )\n",
    "\n",
    "    cr_order_bound_fine = classification_report( picos_fine, base_preds_flattened, digits=4, output_dict=True)\n",
    "    print(f'\\n\\nConfusion matrix for fine-grained ground truth and {i} matches')\n",
    "    report = '{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(cr_order_bound_fine['macro avg']['recall'], cr_order_bound_fine['macro avg']['f1-score'], cr_order_bound_fine['0']['precision'], cr_order_bound_fine['0']['recall'], cr_order_bound_fine['0']['f1-score'], cr_order_bound_fine['1']['precision'], cr_order_bound_fine['1']['recall'], cr_order_bound_fine['1']['f1-score'])\n",
    "    print( report )\n",
    "    \n",
    "    print( '--------------------------------------------------------------------------' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15882a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2b1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75907b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336711a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f5907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
