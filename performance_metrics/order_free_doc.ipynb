{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59401921",
   "metadata": {},
   "source": [
    "# Order-free matching performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89e1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import ast\n",
    "import json\n",
    "\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, auc, roc_curve\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52458f3",
   "metadata": {},
   "source": [
    "## Read order-free candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9a9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "picos = 'I' # ['P', 'I', 'O', 'S']\n",
    "match_level = 'doc' # ['doc', 'sent', 'win_5', 'para']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390e0fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:  ['Biomedical_or_Dental_Material.json', '.nfs0000000011200a4200000003', 'Classification.json', 'Intellectual_Product.json', 'Biologically_Active_Substance.json', 'Diagnostic_Procedure.json', 'Gene_or_Genome.json', 'Finding.json', 'Functional_Concept.json', 'Medical_Device.json', 'Organic_Chemical.json', 'Laboratory__Procedure.json', 'Manufactured_Object.json', 'train_ebm_intervention.json', 'Professional_Society.json', 'Pharmacologic_Substance.json', 'Therapeutic_or_Preventive_Procedure.json', 'train_ebm_intervention_syn.json', 'Biomedical_Occupation_or_Discipline.json', 'Health_Care_Activity.json', 'Idea_or_Concept.json', 'Temporal_Concept.json']\n"
     ]
    }
   ],
   "source": [
    "order_free_dir = f'/mnt/nas2/results/Results/systematicReview/order_free_matching/EBM_PICO_training_matches/order_free/{match_level}/{picos}'\n",
    "order_free_files = os.listdir(order_free_dir)\n",
    "print('Files: ', order_free_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1012ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_free_files.remove('.nfs0000000011200a4200000003')\n",
    "#order_free_files.remove('Finding.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822093ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Biomedical_or_Dental_Material.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 1/21 [00:00<00:15,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Classification.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 2/21 [00:01<00:11,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Intellectual_Product.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 3/21 [00:16<02:09,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Biologically_Active_Substance.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 4/21 [00:20<01:41,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Diagnostic_Procedure.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 5/21 [00:44<03:20, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Gene_or_Genome.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 6/21 [00:45<02:08,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Finding.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 7/21 [02:17<08:22, 35.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Functional_Concept.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 8/21 [02:17<05:19, 24.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Medical_Device.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 9/21 [02:20<03:33, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Organic_Chemical.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 10/21 [02:22<02:19, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Laboratory__Procedure.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 11/21 [02:51<02:58, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Manufactured_Object.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 12/21 [02:52<01:52, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... train_ebm_intervention.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 13/21 [03:00<01:31, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Professional_Society.json\n",
      "Loading file... Pharmacologic_Substance.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 15/21 [03:04<00:41,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Therapeutic_or_Preventive_Procedure.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 16/21 [06:17<04:25, 53.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... train_ebm_intervention_syn.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 17/21 [06:19<02:39, 39.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Biomedical_Occupation_or_Discipline.json\n",
      "Loading file... Health_Care_Activity.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [07:34<00:29, 29.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file... Idea_or_Concept.json\n",
      "Loading file... Temporal_Concept.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [07:35<00:00, 21.67s/it]\n"
     ]
    }
   ],
   "source": [
    "orf_loaded_files = dict()\n",
    "\n",
    "for i in tqdm(order_free_files):\n",
    "    \n",
    "    filpath = f'{order_free_dir}/{i}'\n",
    "    print('Loading file...', i)\n",
    "    with open( filpath, 'r' ) as rf:\n",
    "        orf_i = json.load(rf)\n",
    "        orf_loaded_files[i] = orf_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482441f",
   "metadata": {},
   "source": [
    "## Load order-free candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49e336c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_n_consecutive(numbers, n):\n",
    "    n = 2\n",
    "    if len(numbers) < n: # if the number of partial matches are less than two, \n",
    "        return False\n",
    "    for i in range(len(numbers) - n + 1):\n",
    "        window = numbers[i:i+n]\n",
    "        if max(window) - min(window) != n-1:\n",
    "            continue\n",
    "        if len(set(window)) == n:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eda1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_numbers_consecutive(numbers):\n",
    "    \"\"\"\n",
    "    Check if numbers in a list are consecutive\n",
    "    \n",
    "    Args:\n",
    "        numbers (list[int]): A list of integers\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the numbers are consecutive, False otherwise\n",
    "    \"\"\"\n",
    "    if len(numbers) < 2:\n",
    "        # A single number cannot be considered consecutive\n",
    "        return False\n",
    "    \n",
    "    # Sort the numbers in ascending order\n",
    "    sorted_numbers = sorted(numbers)\n",
    "    \n",
    "    # Check if each number is equal to the previous number plus one\n",
    "    for i in range(1, len(sorted_numbers)):\n",
    "        if sorted_numbers[i] != sorted_numbers[i-1] + 1:\n",
    "            return False\n",
    "    \n",
    "    # If all numbers are consecutive, return True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fcd96ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "\n",
    "set_partial_matches = True\n",
    "set_actual_orf_matches = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3cf821f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orf(v, par, actual_orf):\n",
    "    \n",
    "    orfs = dict()\n",
    "    \n",
    "    for k_i, v_i in v.items():\n",
    "        # print( k_i ) # example: name_15_3, name_9_9, name_12_8\n",
    "        \n",
    "        if 'Inters. (full)' in v_i and len(v_i['Inters. (full)']) > 0:\n",
    "            \n",
    "            full_inters = v_i['Inters. (full)']\n",
    "            # full_inters.keys() = PMIDs\n",
    "            # full_inters.values() = offsets, tokens\n",
    "\n",
    "            for pmid, matches in full_inters.items():\n",
    "                if len(matches['char offs.']) > 1:\n",
    "                    if pmid not in orfs:\n",
    "                        if actual_orf == True:\n",
    "                            #if sorted( matches['word offs.'] ) != matches['word offs.']:\n",
    "                            if are_numbers_consecutive(matches['word offs.'], 2) == False:\n",
    "                                orfs[pmid] = [ ]\n",
    "                                orfs[pmid].extend( matches['word offs.'] )      \n",
    "                        elif actual_orf == False: \n",
    "                            orfs[pmid] = [ ]\n",
    "                            orfs[pmid].extend( matches['word offs.'] )\n",
    "                    else:\n",
    "                        if actual_orf == True:\n",
    "                            #if sorted( matches['word offs.'] ) != matches['word offs.']:\n",
    "                            if are_numbers_consecutive(matches['word offs.'], 2) == False:\n",
    "                                orfs[pmid].extend( matches['word offs.'] )\n",
    "                        elif actual_orf == False:\n",
    "                            orfs[pmid].extend( matches['word offs.'] )\n",
    "\n",
    "\n",
    "        if 'Inters. (partial)' in v_i and len(v_i['Inters. (partial)']) > 0 and (par=='both' or par==True):\n",
    "            \n",
    "            par_inters = v_i['Inters. (partial)']\n",
    "            \n",
    "            for pmid, matches in par_inters.items():\n",
    "                if len(matches['char offs.']) > 1:\n",
    "                    if pmid not in orfs:\n",
    "                        if actual_orf == True:\n",
    "                            #if sorted( matches['word offs.'] ) != matches['word offs.']:\n",
    "                            if are_numbers_consecutive(matches['word offs.'], 2) == False:\n",
    "                                orfs[pmid] = [ ]\n",
    "                                orfs[pmid].extend( matches['word offs.'] )   \n",
    "                        elif actual_orf == False:\n",
    "                            orfs[pmid] = [ ]\n",
    "                            orfs[pmid].extend( matches['word offs.'] ) \n",
    "                    else:\n",
    "                        if actual_orf == True:\n",
    "                            #if sorted( matches['word offs.'] ) != matches['word offs.']:\n",
    "                            if are_numbers_consecutive(matches['word offs.'], 2) == False:\n",
    "                                orfs[pmid].extend( matches['word offs.'] )\n",
    "                        elif actual_orf == False:\n",
    "                            orfs[pmid].extend( matches['word offs.'] )\n",
    "    \n",
    "    return orfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6106b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "orfs_dict = dict()\n",
    "\n",
    "for k,v in orf_loaded_files.items():\n",
    "\n",
    "    orf_fetched = get_orf(v, par = set_partial_matches, actual_orf = set_actual_orf_matches)\n",
    "    orfs_dict[k] = orf_fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "85216866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offsets in Biomedical_or_Dental_Material.json:  269764\n",
      "Offsets in Classification.json:  221903\n",
      "Offsets in Intellectual_Product.json:  6384570\n",
      "Offsets in Biologically_Active_Substance.json:  1747134\n",
      "Offsets in Diagnostic_Procedure.json:  8650791\n",
      "Offsets in Gene_or_Genome.json:  707175\n",
      "Offsets in Finding.json:  34806379\n",
      "Offsets in Functional_Concept.json:  438148\n",
      "Offsets in Medical_Device.json:  2408193\n",
      "Offsets in Organic_Chemical.json:  1226365\n",
      "Offsets in Laboratory__Procedure.json:  9350649\n",
      "Offsets in Manufactured_Object.json:  351652\n",
      "Offsets in train_ebm_intervention.json:  20569\n",
      "Offsets in Professional_Society.json:  1014\n",
      "Offsets in Pharmacologic_Substance.json:  3235747\n",
      "Offsets in Therapeutic_or_Preventive_Procedure.json:  80134307\n",
      "Offsets in train_ebm_intervention_syn.json:  74471\n",
      "Offsets in Biomedical_Occupation_or_Discipline.json:  29481\n",
      "Offsets in Health_Care_Activity.json:  13611371\n",
      "Offsets in Idea_or_Concept.json:  168142\n",
      "Offsets in Temporal_Concept.json:  656504\n"
     ]
    }
   ],
   "source": [
    "# Order the loaded ORF matches in ascending order depending on the number of matches found\n",
    "\n",
    "order_offset_dict = dict()\n",
    "\n",
    "for k,v in orfs_dict.items():\n",
    "    \n",
    "    total_offsets = []\n",
    "    for k_i, v_i in v.items():\n",
    "        total_offsets.extend( v_i )\n",
    "    \n",
    "    order_offset_dict[k] = len(total_offsets)\n",
    "    \n",
    "    print( f'Offsets in {k}: ', len(total_offsets) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d826b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dictionary\n",
    "order_offset_sorteddict = dict(sorted(order_offset_dict.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ad20a657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Professional_Society',\n",
      "'train_ebm_intervention',\n",
      "'Biomedical_Occupation_or_Discipline',\n",
      "'train_ebm_intervention_syn',\n",
      "'Idea_or_Concept',\n",
      "'Classification',\n",
      "'Biomedical_or_Dental_Material',\n",
      "'Manufactured_Object',\n",
      "'Functional_Concept',\n",
      "'Temporal_Concept',\n",
      "'Gene_or_Genome',\n",
      "'Organic_Chemical',\n",
      "'Biologically_Active_Substance',\n",
      "'Medical_Device',\n",
      "'Pharmacologic_Substance',\n",
      "'Intellectual_Product',\n",
      "'Diagnostic_Procedure',\n",
      "'Laboratory__Procedure',\n",
      "'Health_Care_Activity',\n",
      "'Finding',\n",
      "'Therapeutic_or_Preventive_Procedure',\n"
     ]
    }
   ],
   "source": [
    "for i in order_offset_sorteddict.keys():\n",
    "    str2print =  '\\'' + str(i).replace('.json', '') + '\\','\n",
    "    print( str2print )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f3616407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of dictionaries loaded:  21\n"
     ]
    }
   ],
   "source": [
    "print( 'Total number of dictionaries loaded: ', len(order_offset_sorteddict) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12e929",
   "metadata": {},
   "source": [
    "## Load order-bound matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a3afaa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_free_matches(x, orf_offsets):\n",
    "    \n",
    "    labs_modified = []\n",
    "    \n",
    "    for i, (identifier, offs, labs) in enumerate( zip(x.pmid, x.offsets, x.labels) ):\n",
    "             \n",
    "        lab_val = [v for k, v in ast.literal_eval(labs).items()] \n",
    "        off_val = ast.literal_eval(offs) \n",
    "        \n",
    "        if str(identifier) in orf_offsets: \n",
    "            orf_matches =  orf_offsets[ str(identifier) ]\n",
    "            match_indices = [ off_val.index(m) for m in orf_matches ]\n",
    "            for i, l in enumerate(lab_val):\n",
    "                if i in match_indices:\n",
    "                    lab_val[i] = 1\n",
    "                    \n",
    "        labs_modified.append( lab_val )\n",
    "        \n",
    "        \n",
    "    return labs_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed3f5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_int = f'/mnt/nas2/results/Results/systematicReview/order_free_matching/EBM_PICO_training_matches/direct/{picos}/lf_ds_intervention_syn.tsv'\n",
    "ob_int_syn = f'/mnt/nas2/results/Results/systematicReview/order_free_matching/EBM_PICO_training_matches/direct/{picos}/lf_ds_intervetion.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6796b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_int_df = pd.read_csv(ob_int, sep='\\t', header=0)\n",
    "ob_int_syn_df = pd.read_csv(ob_int_syn, sep='\\t', header=0)\n",
    "ob_merged_df = pd.concat([ob_int_df,ob_int_syn_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "02f85f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gt(l):\n",
    "    \n",
    "    labels = l\n",
    "    \n",
    "    if isinstance(labels, str):\n",
    "        labels = ast.literal_eval(labels)\n",
    "        \n",
    "    # convert non-1 fine labels labels to 1's\n",
    "    labels = ['1' if (n != '1' and n != '0') else str(n) for i, n in enumerate(labels) ]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "ob_int_df['i'] = ob_int_df.i.apply(process_gt)\n",
    "ob_int_syn_df['i'] = ob_int_syn_df.i.apply(process_gt)\n",
    "\n",
    "ob_int_df['i_f'] = ob_int_df.i_f.apply(process_gt)\n",
    "ob_int_syn_df['i_f'] = ob_int_syn_df.i_f.apply(process_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "803aa5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch ground truth from the direct matching\n",
    "\n",
    "coarse_int_gt = dict(zip(ob_int_df['pmid'], ob_int_df['i']))\n",
    "fine_int_gt = dict(zip(ob_int_df['pmid'], ob_int_df['i_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e05c1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess order-bound labels\n",
    "\n",
    "def process_ob_labs(l):\n",
    "    \n",
    "    labels = l\n",
    "    \n",
    "    if isinstance( labels, str ):\n",
    "        labels = ast.literal_eval(labels)\n",
    "\n",
    "    labels = [ v for k, v in labels.items() ]\n",
    "    labels = ['0' if n == -1 else str(n) for i, n in enumerate(labels) ]\n",
    "\n",
    "    return labels\n",
    "\n",
    "ob_int_df['labels'] = ob_int_df.labels.apply(process_ob_labs) # order bound matching labels for int source\n",
    "ob_int_syn_df['labels'] = ob_int_syn_df.labels.apply(process_ob_labs) # order bound matching labels for int_syn source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "326e4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch order-bound predictions for merged dataframes\n",
    "\n",
    "ob_preds_merged = dict()\n",
    "\n",
    "ob_int_dict = dict(zip(ob_int_df['pmid'], ob_int_df['labels']))\n",
    "ob_int_syn_dict = dict(zip(ob_int_syn_df['pmid'], ob_int_syn_df['labels']))\n",
    "\n",
    "for k,v in ob_int_dict.items():\n",
    "\n",
    "    if k not in ob_preds_merged:\n",
    "        ob_preds_merged[k] = []\n",
    "        ob_preds_merged[k] = v\n",
    "\n",
    "    else:\n",
    "        old_pred = ob_preds_merged[k]\n",
    "        new_pred = v\n",
    "        \n",
    "        # merge old and new predictions\n",
    "        merged_predictions = [ max( o,n ) for o,n in zip( old_pred, new_pred ) ]\n",
    "        assert len( old_pred ) == len( new_pred ) == len( merged_predictions )\n",
    "        ob_preds_merged[k] = merged_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "98ea93cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4802"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( list(ob_preds_merged.values()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "39cceae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ob_int_syn_dict.items():\n",
    "\n",
    "    if k not in ob_preds_merged:\n",
    "        ob_preds_merged[k] = []\n",
    "        ob_preds_merged[k] = v\n",
    "\n",
    "    else:\n",
    "        old_pred = ob_preds_merged[k]\n",
    "        new_pred = v\n",
    "\n",
    "        # merge old and new predictions\n",
    "        #print( 'merging the new predictions...' )\n",
    "        merged_predictions = [ max( o,n ) for o,n in zip( old_pred, new_pred ) ]\n",
    "        assert len( old_pred ) == len( new_pred ) == len( merged_predictions )\n",
    "        ob_preds_merged[k] = merged_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7b1dd",
   "metadata": {},
   "source": [
    "## merge and calculate evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61079a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(d):\n",
    "    l = [ v for k,v in d.items() ]\n",
    "    l = [item for sublist in l for item in sublist]\n",
    "    l = list(map(int, l))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2c7d9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth : coarse_int_gt, fine_int_gt\n",
    "# ob/direct matching preds :  ob_preds_merged\n",
    "\n",
    "order_bound_preds = flatten( ob_preds_merged )\n",
    "picos_coarse = flatten( coarse_int_gt )\n",
    "picos_fine = flatten( fine_int_gt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2dce611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_preds_ordered( ob_preds, orf_offsets ):\n",
    "    \n",
    "    merged_predictions = dict()\n",
    "    \n",
    "    for k, v in ob_preds.items(): # ob preds are the old preds\n",
    "        \n",
    "        k = str(k)\n",
    "\n",
    "        if k in orf_offsets:\n",
    "            matching_offsets = orf_offsets[k]\n",
    "            old_preds = list( map( int, v ))\n",
    "            new_preds = list( map( int, v ))\n",
    "                             \n",
    "            # add new offsets to the new_preds\n",
    "            for indice in matching_offsets:\n",
    "                new_preds[indice] = 1\n",
    "\n",
    "            merged_predictions[ k ] = new_preds\n",
    "        else:\n",
    "            merged_predictions[ k ] = list( map( int, v ) )\n",
    "    \n",
    "    return merged_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c89ccfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_orf_ordered = list(order_offset_sorteddict.keys())\n",
    "all_orf_ordered.insert( 0, 'order bound' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f012c59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for coarse-grained ground truth and order bound matches\n",
      "0.6077, 0.5335, 0.9295, 0.7394, 0.8236, 0.1635, 0.4761, 0.2434\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and order bound matches\n",
      "0.6438, 0.5252, 0.9565, 0.7385, 0.8335, 0.1352, 0.5492, 0.2169\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Professional_Society.json matches\n",
      "0.6077, 0.5334, 0.9295, 0.7391, 0.8235, 0.1634, 0.4762, 0.2433\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Professional_Society.json matches\n",
      "0.6438, 0.5251, 0.9565, 0.7382, 0.8333, 0.1351, 0.5494, 0.2169\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and train_ebm_intervention.json matches\n",
      "0.6073, 0.5324, 0.9295, 0.7369, 0.8221, 0.1627, 0.4778, 0.2427\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and train_ebm_intervention.json matches\n",
      "0.6435, 0.5240, 0.9566, 0.7360, 0.8319, 0.1345, 0.5510, 0.2162\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Biomedical_Occupation_or_Discipline.json matches\n",
      "0.6072, 0.5318, 0.9295, 0.7355, 0.8212, 0.1623, 0.4788, 0.2424\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Biomedical_Occupation_or_Discipline.json matches\n",
      "0.6434, 0.5234, 0.9566, 0.7347, 0.8311, 0.1341, 0.5522, 0.2158\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and train_ebm_intervention_syn.json matches\n",
      "0.6063, 0.5296, 0.9295, 0.7305, 0.8181, 0.1607, 0.4821, 0.2410\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and train_ebm_intervention_syn.json matches\n",
      "0.6425, 0.5210, 0.9566, 0.7297, 0.8279, 0.1326, 0.5553, 0.2141\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Idea_or_Concept.json matches\n",
      "0.6022, 0.5227, 0.9289, 0.7172, 0.8095, 0.1557, 0.4872, 0.2359\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Idea_or_Concept.json matches\n",
      "0.6383, 0.5140, 0.9563, 0.7166, 0.8193, 0.1282, 0.5599, 0.2086\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Classification.json matches\n",
      "0.6005, 0.5196, 0.9287, 0.7110, 0.8054, 0.1535, 0.4899, 0.2338\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Classification.json matches\n",
      "0.6366, 0.5108, 0.9562, 0.7106, 0.8153, 0.1264, 0.5627, 0.2064\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Biomedical_or_Dental_Material.json matches\n",
      "0.6010, 0.5192, 0.9289, 0.7092, 0.8043, 0.1535, 0.4927, 0.2340\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Biomedical_or_Dental_Material.json matches\n",
      "0.6373, 0.5104, 0.9564, 0.7087, 0.8142, 0.1263, 0.5659, 0.2066\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Manufactured_Object.json matches\n",
      "0.6006, 0.5176, 0.9290, 0.7052, 0.8018, 0.1526, 0.4959, 0.2333\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Manufactured_Object.json matches\n",
      "0.6371, 0.5086, 0.9565, 0.7048, 0.8116, 0.1255, 0.5694, 0.2057\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Functional_Concept.json matches\n",
      "0.5986, 0.5141, 0.9287, 0.6982, 0.7971, 0.1503, 0.4991, 0.2311\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Functional_Concept.json matches\n",
      "0.6350, 0.5050, 0.9564, 0.6978, 0.8069, 0.1235, 0.5723, 0.2032\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Temporal_Concept.json matches\n",
      "0.5961, 0.5102, 0.9283, 0.6908, 0.7921, 0.1479, 0.5014, 0.2284\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Temporal_Concept.json matches\n",
      "0.6324, 0.5011, 0.9561, 0.6905, 0.8019, 0.1213, 0.5743, 0.2003\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Gene_or_Genome.json matches\n",
      "0.5955, 0.5086, 0.9283, 0.6870, 0.7897, 0.1470, 0.5039, 0.2276\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Gene_or_Genome.json matches\n",
      "0.6319, 0.4995, 0.9562, 0.6869, 0.7994, 0.1206, 0.5770, 0.1995\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Organic_Chemical.json matches\n",
      "0.5965, 0.5083, 0.9286, 0.6850, 0.7884, 0.1471, 0.5079, 0.2282\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Organic_Chemical.json matches\n",
      "0.6333, 0.4991, 0.9565, 0.6848, 0.7982, 0.1208, 0.5819, 0.2001\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Biologically_Active_Substance.json matches\n",
      "0.5960, 0.5074, 0.9286, 0.6830, 0.7871, 0.1466, 0.5090, 0.2276\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Biologically_Active_Substance.json matches\n",
      "0.6329, 0.4982, 0.9565, 0.6829, 0.7969, 0.1203, 0.5829, 0.1995\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Medical_Device.json matches\n",
      "0.5958, 0.5044, 0.9288, 0.6753, 0.7820, 0.1454, 0.5163, 0.2269\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Medical_Device.json matches\n",
      "0.6331, 0.4951, 0.9569, 0.6752, 0.7917, 0.1192, 0.5909, 0.1984\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Pharmacologic_Substance.json matches\n",
      "0.5959, 0.5038, 0.9289, 0.6734, 0.7808, 0.1452, 0.5184, 0.2268\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Pharmacologic_Substance.json matches\n",
      "0.6333, 0.4944, 0.9570, 0.6733, 0.7905, 0.1191, 0.5933, 0.1983\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Intellectual_Product.json matches\n",
      "0.5881, 0.4916, 0.9277, 0.6493, 0.7639, 0.1385, 0.5270, 0.2194\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Intellectual_Product.json matches\n",
      "0.6257, 0.4823, 0.9564, 0.6497, 0.7737, 0.1134, 0.6018, 0.1908\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Diagnostic_Procedure.json matches\n",
      "0.5868, 0.4884, 0.9276, 0.6420, 0.7588, 0.1371, 0.5316, 0.2180\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Diagnostic_Procedure.json matches\n",
      "0.6246, 0.4789, 0.9564, 0.6425, 0.7686, 0.1121, 0.6067, 0.1893\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Laboratory__Procedure.json matches\n",
      "0.5857, 0.4865, 0.9274, 0.6380, 0.7560, 0.1362, 0.5333, 0.2170\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Laboratory__Procedure.json matches\n",
      "0.6234, 0.4770, 0.9563, 0.6386, 0.7658, 0.1113, 0.6083, 0.1882\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Health_Care_Activity.json matches\n",
      "0.5803, 0.4775, 0.9266, 0.6197, 0.7427, 0.1321, 0.5409, 0.2123\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Health_Care_Activity.json matches\n",
      "0.6181, 0.4680, 0.9559, 0.6206, 0.7526, 0.1078, 0.6157, 0.1834\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Finding.json matches\n",
      "0.5776, 0.4731, 0.9261, 0.6107, 0.7361, 0.1302, 0.5444, 0.2101\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Finding.json matches\n",
      "0.6155, 0.4636, 0.9557, 0.6117, 0.7460, 0.1061, 0.6193, 0.1812\n",
      "--------------------------------------------------------------------------\n",
      "Confusion matrix for coarse-grained ground truth and Therapeutic_or_Preventive_Procedure.json matches\n",
      "0.5777, 0.4722, 0.9262, 0.6082, 0.7343, 0.1300, 0.5471, 0.2101\n",
      "\n",
      "\n",
      "Confusion matrix for fine-grained ground truth and Therapeutic_or_Preventive_Procedure.json matches\n",
      "0.6159, 0.4627, 0.9559, 0.6093, 0.7442, 0.1060, 0.6225, 0.1811\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "base_preds = dict()\n",
    "base_preds_flattened = []\n",
    "\n",
    "\n",
    "for count, i in enumerate(all_orf_ordered):\n",
    "    \n",
    "    # get offsets and merge with the ob ones\n",
    "    if count == 0:\n",
    "        base_preds = ob_preds_merged\n",
    "        base_preds_flattened =  flatten( ob_preds_merged )\n",
    "    else:\n",
    "        # Add more dicts to orderbound preds and modify base_preds_flattened\n",
    "        updated_of_preds = merge_preds_ordered( base_preds, orfs_dict[i] )\n",
    "        base_preds_flattened =  flatten( updated_of_preds )\n",
    "        base_preds = updated_of_preds\n",
    "    \n",
    "    \n",
    "    # Classification report\n",
    "    cr_order_bound_coarse = classification_report( picos_coarse, base_preds_flattened, digits=4, output_dict=True)\n",
    "    print(f'Confusion matrix for coarse-grained ground truth and {i} matches')\n",
    "    report = '{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(cr_order_bound_coarse['macro avg']['recall'], cr_order_bound_coarse['macro avg']['f1-score'], cr_order_bound_coarse['0']['precision'], cr_order_bound_coarse['0']['recall'], cr_order_bound_coarse['0']['f1-score'], cr_order_bound_coarse['1']['precision'], cr_order_bound_coarse['1']['recall'], cr_order_bound_coarse['1']['f1-score'])\n",
    "    print( report )\n",
    "\n",
    "    cr_order_bound_fine = classification_report( picos_fine, base_preds_flattened, digits=4, output_dict=True)\n",
    "    print(f'\\n\\nConfusion matrix for fine-grained ground truth and {i} matches')\n",
    "    report = '{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(cr_order_bound_fine['macro avg']['recall'], cr_order_bound_fine['macro avg']['f1-score'], cr_order_bound_fine['0']['precision'], cr_order_bound_fine['0']['recall'], cr_order_bound_fine['0']['f1-score'], cr_order_bound_fine['1']['precision'], cr_order_bound_fine['1']['recall'], cr_order_bound_fine['1']['f1-score'])\n",
    "    print( report )\n",
    "    \n",
    "    print( '--------------------------------------------------------------------------' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab588b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64259f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d445b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e7a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb1028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
